{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8079778f",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152d9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backend/sitemap_parser.py\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def normalize_url(url: str) -> str:\n",
    "    url = url.lower().strip() # Remove leading/trailing whitespace and convert to lowercase\n",
    "    if not url.startswith(\"http\"):\n",
    "        url = \"https://\" + url\n",
    "    \n",
    "    if url.startswith(\"https://\") and not url.startswith(\"https://www.\"):\n",
    "        url = url.replace(\"https://\", \"https://www.\", 1)\n",
    "    elif not url.startswith(\"https://www.\"):\n",
    "        url = \"https://www.\" + url\n",
    "    \n",
    "    parsed = urlparse(url)\n",
    "\n",
    "    # Normalize to scheme + netloc only (strip path, params, query, fragment)\n",
    "    normalized_url = urlunparse((parsed.scheme, parsed.netloc, '', '', '', ''))\n",
    "    return normalized_url\n",
    "\n",
    "def fetch_sitemap(url): # Fetch each xml sitemap in one layer.\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'lxml-xml')\n",
    "        return [loc.text for loc in soup.find_all('loc')]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "def parse_sitemap(url): # Parse the sitemap and return a dictionary of URLs. Applies fetch_sitemap to each xml sitemap layer by layer.\n",
    "    locs = fetch_sitemap(url)\n",
    "    if not locs:\n",
    "        return {url: []}\n",
    "    \n",
    "    tree = {}\n",
    "    urls = []\n",
    "\n",
    "    for loc in locs:\n",
    "        if loc.endswith('.xml'):\n",
    "            tree[loc] = parse_sitemap(loc)\n",
    "        else:\n",
    "            urls.append(loc)\n",
    "\n",
    "    if tree and urls:\n",
    "        tree[\"_final_urls\"] = urls\n",
    "        return tree\n",
    "    elif urls:\n",
    "        return urls\n",
    "    else:\n",
    "        return {url: tree}\n",
    "\n",
    "def extract_final_urls(url): # List all URLs in the sitemap.\n",
    "    \n",
    "    url = normalize_url(url)\n",
    "    final_urls = [url]\n",
    "    if not url.endswith('/sitemap.xml'):\n",
    "        url += '/sitemap.xml'\n",
    "    tree = parse_sitemap(url)\n",
    "    \n",
    "\n",
    "    def _walk_tree(node):\n",
    "        if isinstance(node, dict):\n",
    "            for key, value in node.items():\n",
    "                if key == \"_final_urls\" and isinstance(value, list):\n",
    "                    final_urls.extend(value)\n",
    "                else:\n",
    "                    _walk_tree(value)\n",
    "        elif isinstance(node, list):\n",
    "            final_urls.extend([v for v in node if not v.endswith('.xml')])\n",
    "\n",
    "    _walk_tree(tree)\n",
    "\n",
    "    \n",
    "    return final_urls, tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435101c1",
   "metadata": {},
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "994cd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "def scrape(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        if text:\n",
    "            return text\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        elems = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "        text = ' '.join(elem.text for elem in elems).strip()\n",
    "        driver.quit()\n",
    "        return text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "# The output here is an input for the RAG model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995f10af",
   "metadata": {},
   "source": [
    "# RAG model (using Langchain)\n",
    "## Database loader (run everyday to update the database based on recent information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a0ee5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Openai API key successfully imported from .env file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping & indexing (Northlight AI): 100%|██████████| 34/34 [00:27<00:00,  1.24link/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_community.vectorstores import SupabaseVectorStore\n",
    "from supabase import create_client, Client\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import getpass\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_core.documents import Document\n",
    "# Load environment variables from .env file\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "else:\n",
    "  print(f\"Openai API key successfully imported from .env file.\")\n",
    "  #print(f\"Key: {os.environ.get('OPENAI_API_KEY')}\")\n",
    "\n",
    "supabase_url = os.environ.get(\"SUPABASE_URL\")\n",
    "supabase_key = os.environ.get(\"SUPABASE_SERVICE_KEY\")\n",
    "supabase: Client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")\n",
    "\n",
    "vector_store = SupabaseVectorStore(\n",
    "    client=supabase,\n",
    "    embedding=embeddings,\n",
    "    table_name=\"nlai_content\",  # You can change this\n",
    "    query_name=\"nlai_match_documents\"  # Needs to be created in Supabase SQL\n",
    ")\n",
    "\n",
    "def RAG_scraper_loader(company_name, website):\n",
    "\n",
    "    # Clean the table before loading new data\n",
    "    supabase.table(\"nlai_content\").delete().not_.is_(\"id\", None).execute()\n",
    "\n",
    "    # Extract sitemap URLs\n",
    "    url_list, tree = extract_final_urls(website)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    for link in tqdm(url_list, desc=f\"Scraping & indexing ({company_name})\", unit=\"link\"):\n",
    "        # Scrape the URL\n",
    "        text = scrape(link)\n",
    "        if not text.strip():\n",
    "            continue  # skip empty pages\n",
    "\n",
    "        # Create metadata and document content\n",
    "        metadata={\n",
    "                \"source\": str(link),\n",
    "                \"website\": str(website)\n",
    "            }\n",
    "\n",
    "        docs = Document(\n",
    "            page_content=text,\n",
    "            metadata=metadata\n",
    "        )\n",
    "        \n",
    "        # Chunking\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=500)\n",
    "        chunks = text_splitter.split_documents([docs])\n",
    "\n",
    "        # Index chunks and store in Supabase\n",
    "        for chunk in chunks:\n",
    "            # Generate embedding\n",
    "            vector = embeddings.embed_query(chunk.page_content)\n",
    "\n",
    "            # Insert into Supabase\n",
    "            supabase.rpc(\"insert_webcontent_nlai\", {\n",
    "                \"content\": chunk.page_content,\n",
    "                \"metadata\": metadata,\n",
    "                \"embedding\": vector  # list of floats; pgvector input accepted here\n",
    "            }).execute()\n",
    "\n",
    "company_name = \"Northlight AI\"\n",
    "website = \"https://northlightai.com\"\n",
    "RAG_scraper_loader(company_name, website)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddcf70f",
   "metadata": {},
   "source": [
    "## Step 2) Retrieval and Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a82b7",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1ba7c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "## Prompt - custom\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use five sentences maximum and keep the answer as concise as possible.\n",
    "Always start the answer with a sentence like \"Thanks for asking question about North Light AI!\"; but be innovative and each time use a similar welcoming message.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "## State and Nodes\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "## Compile the graph\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6bb800",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f16002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot():\n",
    "    question = input(\"What would you like to know? \")\n",
    "    state = graph.invoke({\"question\": question})\n",
    "    return print(state[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "376f63cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for asking a question about North Light AI! Andrew Mitchell is the Co-Founder and President of North Light AI, where he is focused on building a collaborative Innovation Lab for practical AI adoption in industry and academia. He has held leadership roles, including Associate Director at the University of New Hampshire's Center for Business Analytics. His expertise lies in leveraging AI tools to address real-world challenges and enhance business growth, especially for small to medium-sized organizations. Andrew's strategic vision is aimed at creating tangible impacts in today's digital landscape.\n"
     ]
    }
   ],
   "source": [
    "chatbot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
